[description]
	I wrote clipodder to be a alternative to bashpodder, and gpo.
	Bashpodder tends to download too much extra "stuff" along with the podcast
	which is quite annoying. Gpo (from gpodder) has too many deps, 
	and uses a sql database backend.
	
	
[deps]
	python2
	python-feedparser [5.1] (On ubuntu use "easy_install feedparser", other systems may vary)
	pycurl (python-pycurl)

[notes]
	The config will be initially created the first time you run
	clipodder. The config file is located at "~/.clipodder/config".
	Clipodder's downloads are based of the files themselves, 
	so don't delete/rename any of them (until it has exceeded 
	the number of downloads_per_url for that podcast). 
	
	Example
	If downloads_per_url is set to 2, it will download the 
	first 2 podcasts from that feed. Eventually you will 
	have 3, 4, 5, etc... videos. If you have 5 downloaded you can 
	delete the oldest 3, and be good to go. 


	You can now download other media types aswell.
	Simply add "pdf", "html", etc... to the end of the url
	
	Urls need to be named in this type of fashon:

	1 = http://someurl.org/feed pdf html 
	2 = http://someurl.org/feed audio video
	foo = http://someurl.org/feed.xml <-- dot xml is ok
	bar = http://someurl.org/feed audio <-- only audio
	
	(It just needs to be unique or ConfigParser will 
	not pick up on it)


	It will now print to the console in color (if enabled
	in the config file).

	[NEW] You can now add path=/directory/here and name=MyDownloadFolder. 
	Do not use spaces with name=

	[NEW] Functionality has been added to delete old files.  This
	can be controlled with the delete_if_more_than option.  This can
	be applied to the entire config file, as well as individual urls.

	Its best to add this to crontab, maybe sometime early in 
	the morning before you wake up.

[config explanation]

	[options explanation]

		- download_dir: The directory that the podcasts will be downloaded to.
		                The podcasts from each URL will go into different directories.

		- downloads_per_url: The number of items from each URL that will be downloaded.
		                     For instance, if a URL contains 100 items, but
		                     downloads_per_url is only 40, then only the 40 most recent
		                     items will be downloaded.

		- use_colors: This determines whether or not colors will be used in the output.
		              Colors will not be used when not printing to the console, even
		              if this is "true".

		- delete_if_more_than: The number of items to keep for each URL.  For instance, if
		                       this is 25, then anything more than 25 items will be
		                       deleted.  The oldest items are deleted first, based
		                       on the file's mtime.  If this is "false" or 0, then
		                       nothing will ever be deleted.


	[urls explanation]

		- path: This path will be used like the download_dir in the [options] section.
		
		- name: This will be used as the name of the directory that contains the actual
		        items from the url.  For instance, if you want to download audio
		        files from a podcast into /some/path/to/TheFolder/, you can use
		        a url item that looks like this:

		foo = http://www.example.net/feed.xml audio path=/some/path/to name=TheFolder

		- delete_if_more_than: This overrides the value from the options section.

		- other values: "audio", "html", "video", "pdf" and other file types can
		                be specified in order to only download these file types from
		                this feed.  This is useful if you only want the mp3 files
		                from a feed, but don't want any video files that also come
		                in the feed.
